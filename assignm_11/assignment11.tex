\documentclass[fontsize=12pt,a4paper]{scrartcl}
 
% Das Prozent\item Zeichen leitet einen Kommentar ein,
% es hilft ebenso, im Text Leerzeichen zu unterbinden.
 
% fontsize=12pt  Schriftgroesse in 10, 11 oder 12 Punkt
% a4paper        Papierformat ist hier A4
% landscape      Querformat wird natÃƒÂ¼rlich unterstÃƒÂ¼tzt ;\item )
% parskip        Absatzabstand anstatt EinzÃƒÂ¼ge
% draft          Der Entwurfsmodus deckt SchwÃƒÂ¤chen auf
% {scrartcl}     Die Dokumentenklasse book, report, article
%                oder fÃƒÂ¼rs deutsche scrbook, scrreprt, scrartcl
 
%\usepackage[ngerman]{babel} % Deutsche Sprachanpassungen
\usepackage[T1]{fontenc}    % Silbentrennung bei Sonderzeichen
\usepackage[latin1]{inputenc} % Direkte Angabe von Umlauten im Dokument.
                            % Wenn Sie an einem Mac sitzen,verwenden
                            % Sie ggf. Ã¢Â€ÂžmacceÃ¢Â€Âœ anstatt Ã¢Â€Âžutf8Ã¢Â€Âœ.
 
\usepackage{textcomp}       % ZusÃƒÂ¤tzliche Symbolzeichen
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\lstset{tabsize=4, showspaces=false}


\title{Machine Learning SS2013}
\subtitle{Ulrike von Luxburg \\ Assignment 11}
\author{Arne Schröder \and Falk Oswald \and Angel Bakardzhiev}
 
\date{\today}               % \today setzt das heutige Datum
 
\begin{document}
\maketitle                  % Titelei erzeugen
% \tableofcontents            % Inhaltsverzeichnis anlegen

\section*{Exercise 1: Beispiel für Anwendung von Reinforcement Learning}

\subsection*{Automatischer Pfannkuchenwender}

\begin{description}
\item [States] Positionen, Geschwindigkeit von Pfanne und Pfannkuchen
\item [Actions] Änderung der Geschwindikeit der Pfanne
\item [Rewards-Funktion] Entfernung der Lande-Position des Pfannkuchens vom Zentru der Pfanne. Drehung des Pfannkuchens (Hat er sich gedreiht, wie oft?): Position geht über Fauß-Funktion mit kleinem Faktor ein. Einmaliges Wenden bringt 1000, zweimaliges Wenden 1100 usw.
\item [State-Space] 3d, Drehung: 3d, Geschwindigkeit:, das ganze für Pfanne und Kuchen: 24d.
\end{description}

\subsection*{Automatischer Staubsaugerroboter}

\begin{description}
\item [States] Position, Ausrichtung (Orientierung), abgefahrene Fläche
\item [Actions] Zu nächster Position fahren
\item [Rewards-Funktion] Menge des übrig gebliebenen Staubs stark negativ berücksichtigen und die Zeit, die benötigt wird, leicht negativ.
\item [State-Space] Position: 2d, Orientierung: 1d, Abgefahrene Fläche (n*m Rasterpunkte (0= nicht besucht, 1 = besucht)
\end{description}

\subsection*{Lernende Künstliche Intelligenz für Schach}
\begin{description}
\item [States] Positionen der Figuren
\item [Actions] Bewegen der Figuren
\item [Rewards-Funktion] Gewichtete Anzahl der Figuren, Bewegungsmöglichkeiten, Schach(matt)
\item [State-Space] 64d (Für jedes Feld, welche Figur drauf steht)
\end{description}

\section*{RL-Ansatz}
Nein.

RL ist nicht angebracht, wenn 100 \% zuverlässige Anforderungen gestellt werden (bsp. Flugzeug fliegen), da es keine Garantie gibt, dass RL zu irgend einem zeitpunkt nicht scheitern wird.

RL kann ebenfalls nicht angewendet werden, wenn ein State nicht eindeutigt bestimmt werden kann oder wenn keine gute Reward Funktion definiert werden kann.

\section*{Exercise 2: Grid-world V-function}

\subsection*{Optimale Policy}
See slide 66 c)

\section*{Exercise 3: Bellman Equation for the $Q$-function}

$Q^\pi(s, a)$ gives the value of an action $a$ when being in state $s$ using the policy $\pi$.

To calculate this, we consider all possible states, that follow $s'$ with their probability $P^a_{ss'}$.

\[
  Q^\pi(s, a) = \sum_{s'} P^a_{ss'} (\dots)
\]

For each of these follow up states, we consider the expected rewards $R^a_{ss'}$ and the value which could be obtained in following actions $a'$: $Q^\pi(s', a')$. These are weighted by the policy $\pi$ given:

\[
  Q^\pi(s, a) = \sum_{s'} P^a_{ss'} \left(R^a_{ss'} + \sum_{a'} \pi(s', a') Q^\pi(s', a') \right)
\]

\section*{Exercise 4: Maze}

Wird für jeden beliebigen Durchlauf durch das Labyrinth der gleiche Reward vergeben, so ist in diesem Sinne auch jeder Durchlauf gleich gut (solange er nicht in einer Endlosschleife hängen bleibt). Wir würden jedoch einen zielstrebigen Durchlauf, welcher weniger Schritte benötigt, als besser ansehen.

Daher muss hier die Reward-Funktion dahingehend geändert werden, dass längere Durchläufe weniger Reward erbringen als kurze. Beispielsweise dadurch, dass die +1 durch die Anzahl der Schritte geteilt wird, oder aber eine fixe Zahl (beispielsweise 100) um die Anzahl der Schritte verringert wird.
\end{document}